seed_everything: 42
trainer:
  enable_checkpointing: true
  accelerator: auto
  devices: auto
  max_epochs: 100
#  gradient_clip_val: 1.0
  sync_batchnorm: false
  precision: 32
  enable_model_summary: true
  weights_save_path: null
  num_sanity_val_steps: 2
  benchmark: true
  logger:
    class_path: pytorch_lightning.loggers.WandbLogger
    init_args:
      project: ot-vae-lightning
      entity: gip
      dir: dad
      log_model: true
      name: sweep-first_kl-deep-1
model:
  expansion: 1
  learning_rate: 1e-3
  lr_sched_metric: fid
  conditional: false
  checkpoints: null
  ce_coeff: 1e-3
  metrics:
    class_path: torchmetrics.MetricCollection
    init_args:
      metrics:
        psnr: torchmetrics.image.psnr.PeakSignalNoiseRatio
        fid: ot_vae_lightning.metrics.fid.FrechetInceptionDistance
  encoder:
    class_path: ot_vae_lightning.networks.ViT
    init_args:
      dim: 128
      depth: 4
      heads: 8
      n_embed_tokens: 0
      output_tokens: input
  decoder:
    class_path: ot_vae_lightning.networks.ViT
    init_args:
      depth: 4
      heads: 8
      n_embed_tokens: 0
      n_input_tokens: null    # linked to model.encoder.total_num_tokens (CLI)
      output_tokens: input
      patch_to_embed: false
      embed_to_patch: true
  autoregressive_decoder:
    class_path: ot_vae_lightning.networks.vit.AutoRegressive
    init_args:
      vocab_size: null        # linked to model.vocabulary.num_embeddings (CLI)
      depth: 2
      heads: 8
      n_embed_tokens: 0
      n_input_tokens: null    # linked to model.encoder.total_num_tokens (CLI)
      output_tokens: input
      patch_to_embed: false
      embed_to_patch: false
      causal_mask: true
  prior_kwargs:
    permute: false
  vocabulary:
    class_path: ot_vae_lightning.prior.codebook.CodebookPrior
    init_args:
      num_embeddings: 8192
      latent_size: null       # linked to model.encoder.out_size (CLI)
      embed_dims: [2]         # token dim
      similarity_metric: cosine
      separate_key_values: false
      topk: null
      mode: mean                # 'mean', 'sample', 'argmax'
      loss: first_kl              # 'kl', 'first_kl', 'l2'
      temperature: 1e-3
      loss_coeff: 1e-3
      annealing_steps: 0
data:
  class_path: ot_vae_lightning.data.CIFAR10
  init_args:
    train_val_split: 0.9
    seed: 42
    train_batch_size: 32
    val_batch_size: 128
    test_batch_size: 128
    num_workers: 8
